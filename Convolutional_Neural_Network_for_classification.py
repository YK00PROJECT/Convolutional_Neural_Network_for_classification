# -*- coding: utf-8 -*-
"""Convolutional_Neural_Network_for_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z6Xa3PlncDU16KB_aG8YXciOW2lq9WIg

# Importing Libraries
"""

import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
import os
from shutil import copyfile
from sklearn.model_selection import train_test_split
import numpy as np

tf.__version__

from google.colab import drive
drive.mount('/content/gdrive',force_remount = True)

# Get the list of image files
data_directory = '/content/gdrive/My Drive/raw-img'
image_files = [f for f in os.listdir(data_directory) if os.path.isfile(os.path.join(data_directory, f))]

for item in os.listdir(data_directory):
    print(item)

data_directory = '/content/gdrive/My Drive/raw-img'
animal_folders = ["scoiattolo", "cane", "ragno", "mucca", "gatto", "gallina", "elefante", "farfalla", "cavallo", "pecora"]

# New directories for train, validation, and test sets
train_dir = os.path.join(data_directory, 'train')
val_dir = os.path.join(data_directory, 'val')
test_dir = os.path.join(data_directory, 'test')

os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

for animal_folder in animal_folders:
    path = os.path.join(data_directory, animal_folder)
    if os.path.exists(path):
        print(f"{animal_folder} exists with {len(os.listdir(path))} files.")
    else:
        print(f"{animal_folder} does not exist.")
for animal_folder in animal_folders:
    # Explicitly create sub-directories in train, val, and test directories for this animal
    train_animal_dir = os.path.join(train_dir, animal_folder)
    val_animal_dir = os.path.join(val_dir, animal_folder)
    test_animal_dir = os.path.join(test_dir, animal_folder)

        # Check the number of files copied
    print(f"Number of files in {train_animal_dir}: {len(os.listdir(train_animal_dir))}")
    print(f"Number of files in {val_animal_dir}: {len(os.listdir(val_animal_dir))}")
    print(f"Number of files in {test_animal_dir}: {len(os.listdir(test_animal_dir))}")



print(os.listdir(train_dir))
print(os.listdir(val_dir))
print(os.listdir(test_dir))

# Perform stratified split for this animal's images
train_files, temp_files = train_test_split(animal_folders, test_size=0.3, random_state=42)
val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)

# Create sub-directories in train, val, and test directories for this animal
train_animal_dir = os.path.join(train_dir, animal_folder)
val_animal_dir = os.path.join(val_dir, animal_folder)
test_animal_dir = os.path.join(test_dir, animal_folder)

os.makedirs(train_animal_dir, exist_ok=True)
os.makedirs(val_animal_dir, exist_ok=True)
os.makedirs(test_animal_dir, exist_ok=True)

# Extensions of image files (you can add more if needed)
image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']

# Get list of all image files for this animal
animal_image_files = [f for f in os.listdir(os.path.join(data_directory, animal_folder))
                      if os.path.isfile(os.path.join(data_directory, animal_folder, f))
                      and any(f.lower().endswith(ext) for ext in image_extensions)]

print(os.listdir(train_dir))
print(os.listdir(val_dir))
print(os.listdir(test_dir))

"""# Data Preprocessing

preprocessing the training test (To avoid overfitting)
rescale is the featurecsaling and the class mode is binary but for more than two classes, one can use categorical
"""

train_dir = '/content/gdrive/My Drive/raw-img/train'
val_dir = '/content/gdrive/My Drive/raw-img/val'
test_dir = '/content/gdrive/My Drive/raw-img/test'

from keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    brightness_range=[0.5, 1.5],
    fill_mode='nearest'
)


training_set = train_datagen.flow_from_directory(
    train_dir,
    target_size=(64, 64),
    batch_size=128,
    class_mode='categorical'  # Changed to 'categorical' because you have more than 2 classes
)

"""Preprocessing Validation Set"""

val_datagen = ImageDataGenerator(rescale=1./255)

validation_set = val_datagen.flow_from_directory(
    val_dir,
    target_size=(64, 64),
    batch_size=128,
    class_mode='categorical'  # Changed to 'categorical'
)

"""Preprocessing Test Set"""

test_datagen = ImageDataGenerator(rescale=1./255)

test_set = test_datagen.flow_from_directory(
    test_dir,
    target_size=(64, 64),
    batch_size=128,
    class_mode='categorical'  # Changed to 'categorical'
)

"""# Building the CNN"""

cnn = tf.keras.models.Sequential()

# First Convolutional Layer
cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=(64, 64, 3)))
cnn.add(tf.keras.layers.BatchNormalization())
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# Second Convolutional Layer
cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
cnn.add(tf.keras.layers.BatchNormalization())
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# Third Convolutional Layer
cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
cnn.add(tf.keras.layers.BatchNormalization())
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# Fourth Convolutional Layer
cnn.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu'))
cnn.add(tf.keras.layers.BatchNormalization())
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
cnn.add(tf.keras.layers.Dropout(0.5))

# Flattening
cnn.add(tf.keras.layers.Flatten())

# Fully Connected Layer
cnn.add(tf.keras.layers.Dense(units=512, activation='relu'))
cnn.add(tf.keras.layers.BatchNormalization())
cnn.add(tf.keras.layers.Dropout(0.5))

# Output Layer
cnn.add(tf.keras.layers.Dense(units=10, activation='softmax'))

"""# Compiling CNN"""

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
cnn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)

history = cnn.fit(
    x=training_set,
    validation_data=validation_set,
    epochs=25,
    steps_per_epoch=len(training_set),
    validation_steps=len(validation_set),
    callbacks=[early_stop]
)

"""# Prediction"""

# Evaluating the CNN on the test set
results = cnn.evaluate(test_set, steps=len(test_set))

test_loss = results[0]
test_accuracy = results[1]

# Printing the test results
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

predictions = cnn.predict(test_set, steps=len(test_set))
predicted_labels = np.argmax(predictions, axis=1)

from tensorflow.keras.preprocessing import image

# Specify the path to your test image
img_path = '/content/gdrive/My Drive/raw-img/test/pecora/e835b90e2bfc013ed1584d05fb1d4e9fe777ead218ac104497f5c978a6ebb3bf_640.jpg'

# Load and preprocess the image
test_image = image.load_img(img_path, target_size=(64,64,3)) # Change this if grayscale
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis=0)
test_image = test_image/255.0 # Assuming you normalized training images in the range [0,1]

# Make the prediction
result = cnn.predict(test_image)

# Determine the predicted class (if you have more than two classes)
predicted_class = np.argmax(result, axis=-1)
class_labels = list(training_set.class_indices.keys())
print('Prediction:', class_labels[predicted_class[0]])

